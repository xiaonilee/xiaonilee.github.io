{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow2class1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6N7YnLeIQM0h"
      },
      "source": [
        "# Class1: 神经网络计算"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkDKnEMRTMvf"
      },
      "source": [
        "## Prereisites\n",
        "\n",
        "### Setup virtul environment\n",
        "\n",
        "- `python3.7` + `TensorFlow2.1.0` + `sklearn` + `pandas` + `matplotlib`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmcNF7pajtFN"
      },
      "source": [
        "## TensorFlow2 安装\n",
        "- 安装Anaconda Python 3.7\n",
        "- TensorFlow2安装\n",
        "  - conda create -n TF2.1 python=3.7\n",
        "  - conda active TF2.1\n",
        "  - conda install cudatoolkit=10.1\n",
        "  - conda install cudnn=7.6\n",
        "  - pip install TensorFlow==2.1.0\n",
        "    - python\n",
        "    - import TensorFlow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrOzlmGscMOM"
      },
      "source": [
        "## 人工智能三学派\n",
        "- 行为主义\n",
        "- 符号主义\n",
        "- 连接主义\n",
        "  - 神经网络\n",
        "  - 计算机仿真神经网络连接关系\n",
        "    - 准备数据:'特征-标签'数据\n",
        "    - 搭建网络:神经网络结构\n",
        "    - 优化参数:训练网络获取最佳参数\n",
        "    - 应用网络:输出分类，或输出预测结果"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Suz4XOgscWBG"
      },
      "source": [
        "## 神经网络设计过程\n",
        "- 分类目标:\n",
        "  - **0**狗尾草Iris\n",
        "  - **1**杂色Iris: 花萼长>花萼宽 and 花瓣长/花瓣宽>2\n",
        "  - **2**弗吉尼亚Iris\n",
        "- 采集大量**数据对**作为**数据集**: 输入特征(花萼长，花萼宽，花瓣长，花瓣宽), 人工标定的标签(对应的类别)\n",
        "- 将已有数据集喂入搭建好的**神经网络结构**\n",
        "- **随机初始化**所有参数, 然后反向传播进行网络**优化参数**,**得到模型**\n",
        "  - 损失函数输出最小(预测值与标准答案之间的差距)，得到所有的最优参数\n",
        "    - 梯度下降法\n",
        "      - 设置合适的学习率(超参数): 不可以过大或者过小\n",
        "- 读入新输入特征(待测)\n",
        "- 输出识别结果: 所属分类\n",
        "\n",
        "### Iris分类的神经网络优化参数\n",
        "- 计算最优w和b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDphoNM4QCbx",
        "outputId": "3a91cfdf-8784-4460-a994-a54ad7bd144a"
      },
      "source": [
        "# p13_backpropagation.py\n",
        "import tensorflow as tf\n",
        "\n",
        "w = tf.Variable(tf.constant(5, dtype=tf.float32))   # tf.Variable()定义变量为可训练(可自更新)属性\n",
        "lr = 0.2\n",
        "# lr = 0.001\n",
        "# lr = 0.999\n",
        "\n",
        "epoch = 40\n",
        "\n",
        "for epoch in range(epoch):  # for epoch 定义顶层循环，表示对数据集循环epoch次，此例数据集数据仅有1个w,初始化时候constant赋值为5，循环40次迭代。\n",
        "    with tf.GradientTape() as tape:  # with结构到grads框起了梯度的计算过程。\n",
        "        loss = tf.square(w + 1)\n",
        "    grads = tape.gradient(loss, w)  # .gradient函数告知谁对谁求导\n",
        "\n",
        "    w.assign_sub(lr * grads)  # .assign_sub 对变量做自减 即：w -= lr*grads 即 w = w - lr*grads\n",
        "    print(\"After %s epoch,w is %f,loss is %f\" % (epoch, w.numpy(), loss))\n",
        "\n",
        "# lr初始值：0.2   请自改学习率  0.001  0.999 看收敛过程\n",
        "# 最终目的：找到 loss 最小 即 w = -1 的最优参数w"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After 0 epoch,w is 2.600000,loss is 36.000000\n",
            "After 1 epoch,w is 1.160000,loss is 12.959999\n",
            "After 2 epoch,w is 0.296000,loss is 4.665599\n",
            "After 3 epoch,w is -0.222400,loss is 1.679616\n",
            "After 4 epoch,w is -0.533440,loss is 0.604662\n",
            "After 5 epoch,w is -0.720064,loss is 0.217678\n",
            "After 6 epoch,w is -0.832038,loss is 0.078364\n",
            "After 7 epoch,w is -0.899223,loss is 0.028211\n",
            "After 8 epoch,w is -0.939534,loss is 0.010156\n",
            "After 9 epoch,w is -0.963720,loss is 0.003656\n",
            "After 10 epoch,w is -0.978232,loss is 0.001316\n",
            "After 11 epoch,w is -0.986939,loss is 0.000474\n",
            "After 12 epoch,w is -0.992164,loss is 0.000171\n",
            "After 13 epoch,w is -0.995298,loss is 0.000061\n",
            "After 14 epoch,w is -0.997179,loss is 0.000022\n",
            "After 15 epoch,w is -0.998307,loss is 0.000008\n",
            "After 16 epoch,w is -0.998984,loss is 0.000003\n",
            "After 17 epoch,w is -0.999391,loss is 0.000001\n",
            "After 18 epoch,w is -0.999634,loss is 0.000000\n",
            "After 19 epoch,w is -0.999781,loss is 0.000000\n",
            "After 20 epoch,w is -0.999868,loss is 0.000000\n",
            "After 21 epoch,w is -0.999921,loss is 0.000000\n",
            "After 22 epoch,w is -0.999953,loss is 0.000000\n",
            "After 23 epoch,w is -0.999972,loss is 0.000000\n",
            "After 24 epoch,w is -0.999983,loss is 0.000000\n",
            "After 25 epoch,w is -0.999990,loss is 0.000000\n",
            "After 26 epoch,w is -0.999994,loss is 0.000000\n",
            "After 27 epoch,w is -0.999996,loss is 0.000000\n",
            "After 28 epoch,w is -0.999998,loss is 0.000000\n",
            "After 29 epoch,w is -0.999999,loss is 0.000000\n",
            "After 30 epoch,w is -0.999999,loss is 0.000000\n",
            "After 31 epoch,w is -1.000000,loss is 0.000000\n",
            "After 32 epoch,w is -1.000000,loss is 0.000000\n",
            "After 33 epoch,w is -1.000000,loss is 0.000000\n",
            "After 34 epoch,w is -1.000000,loss is 0.000000\n",
            "After 35 epoch,w is -1.000000,loss is 0.000000\n",
            "After 36 epoch,w is -1.000000,loss is 0.000000\n",
            "After 37 epoch,w is -1.000000,loss is 0.000000\n",
            "After 38 epoch,w is -1.000000,loss is 0.000000\n",
            "After 39 epoch,w is -1.000000,loss is 0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RntEkUyzQrC7"
      },
      "source": [
        "## 张量生成\n",
        "\n",
        "### 创建张量"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYYBPmEJQsOD",
        "outputId": "5fa9bdb1-710a-484e-b23c-71f2a0291048"
      },
      "source": [
        "# p17_constant.py\n",
        "import tensorflow as tf\n",
        "\n",
        "a = tf.constant([1, 5], dtype=tf.int64)\n",
        "print(\"a:\", a)\n",
        "print(\"a.dtype:\", a.dtype)\n",
        "print(\"a.shape:\", a.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a: tf.Tensor([1 5], shape=(2,), dtype=int64)\n",
            "a.dtype: <dtype: 'int64'>\n",
            "a.shape: (2,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJ7r7CDKQgQ_"
      },
      "source": [
        "### 转化成张量"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCevLWrNOByY",
        "outputId": "9ec0400c-1b79-4c1e-80fd-968e2bc4c5fb"
      },
      "source": [
        "# p18_convert_to_tensor.py\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "a = np.arange(0, 5)\n",
        "b = tf.convert_to_tensor(a, dtype=tf.int64)\n",
        "print(\"a:\", a)\n",
        "print(\"b:\", b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a: [0 1 2 3 4]\n",
            "b: tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjM2TIfWQ1sT"
      },
      "source": [
        "### 不同维度"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MrXe_xDQ223",
        "outputId": "6d33ea44-4b37-412a-952d-e3e585cef41b"
      },
      "source": [
        "# p19_zeros_ones_fill.py\n",
        "import tensorflow as tf\n",
        "\n",
        "a = tf.zeros([2, 3])    # 二维，2行3列\n",
        "b = tf.ones(4)          # 一维\n",
        "c = tf.fill([2, 2], 9)  # 多维 2行2列，用元素为9\n",
        "print(\"a:\", a)\n",
        "print(\"b:\", b)\n",
        "print(\"c:\", c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a: tf.Tensor(\n",
            "[[0. 0. 0.]\n",
            " [0. 0. 0.]], shape=(2, 3), dtype=float32)\n",
            "b: tf.Tensor([1. 1. 1. 1.], shape=(4,), dtype=float32)\n",
            "c: tf.Tensor(\n",
            "[[9 9]\n",
            " [9 9]], shape=(2, 2), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKoYgjARSJOG"
      },
      "source": [
        "### 生成值"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRZQ9ryQSJ2g",
        "outputId": "84063627-09d2-4f8f-baf9-6aa42b464188"
      },
      "source": [
        "# p21_random.normal.py\n",
        "import tensorflow as tf\n",
        "\n",
        "d = tf.random.normal([2, 2], mean=0.5, stddev=1)\n",
        "print(\"d:\", d)\n",
        "\n",
        "# tf.random.truncated_normal() 可以保证生成值在均值附近(均值+2*标准差； 均值-2*标准差)\n",
        "e = tf.random.truncated_normal([2, 2], mean=0.5, stddev=1)\n",
        "print(\"e:\", e)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "d: tf.Tensor(\n",
            "[[-1.0630833   1.810489  ]\n",
            " [ 0.38028917  0.74174047]], shape=(2, 2), dtype=float32)\n",
            "e: tf.Tensor(\n",
            "[[0.01437405 1.9270004 ]\n",
            " [1.3503292  1.1851509 ]], shape=(2, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jchZjI3aSMRW"
      },
      "source": [
        "### 生成均匀随机数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiD-iTV1SRvL",
        "outputId": "0ded9d2e-e1c9-4b88-da39-c2ff990e35d2"
      },
      "source": [
        "# p22_random.uniform.py\n",
        "import tensorflow as tf\n",
        "\n",
        "# 生成均匀分布随机数，前闭后开[minval, maxval)\n",
        "f = tf.random.uniform([2, 2], minval=0, maxval=1)\n",
        "print(\"f:\", f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "f: tf.Tensor(\n",
            "[[0.37125707 0.92167485]\n",
            " [0.48856616 0.9187217 ]], shape=(2, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXD4YJb9SUyE"
      },
      "source": [
        "### 强制数据类型的转换"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJd5oi2FSbw_",
        "outputId": "a2505b83-c0c0-4556-9d23-478febdd4ddf"
      },
      "source": [
        "# p23_cast_reduce_minmax.py\n",
        "import tensorflow as tf\n",
        "\n",
        "x1 = tf.constant([1., 2., 3.], dtype=tf.float64)\n",
        "print(\"x1:\", x1)\n",
        "\n",
        "# 强制转换数据类型\n",
        "x2 = tf.cast(x1, tf.int32)\n",
        "print(\"x2\", x2)\n",
        "print(\"minimum of x2：\", tf.reduce_min(x2))\n",
        "print(\"maxmum of x2:\", tf.reduce_max(x2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x1: tf.Tensor([1. 2. 3.], shape=(3,), dtype=float64)\n",
            "x2 tf.Tensor([1 2 3], shape=(3,), dtype=int32)\n",
            "minimum of x2： tf.Tensor(1, shape=(), dtype=int32)\n",
            "maxmum of x2: tf.Tensor(3, shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcRajpz5GjG1"
      },
      "source": [
        "## 常用函数\n",
        "\n",
        "- reduce_mean,reduce_sum\n",
        "  - axis=0:纵向，经度方向\n",
        "  - axis=1:横向，维度方向\n",
        "- 维度相同的张量可以做四则运算\n",
        "  - add, subtract, multiply, divide.\n",
        "- 平方, 次方, 开方\n",
        "  - square, pow, sqrt\n",
        "- 矩阵乘\n",
        "  - tf.matmul\n",
        "- 将(特征, 标签)进行配对\n",
        "  - tf.data.Dataset.from_tensor_slices((features, labels))\n",
        "- 梯度\n",
        "  - tf.GradientTape\n",
        "- 枚举, python内置函数\n",
        "  - enumerate\n",
        "- 独热编码,作为标签\n",
        "  - tf.one_hot\n",
        "  - 1: 表示是\n",
        "  - 0: 表示非\n",
        "- 使输出符合概率分布\n",
        "  -tf.nn.softmax  -->n分类n输出调用softmax()函数\n",
        "- 自减\n",
        "  - assign_sub\n",
        "- 张量方向最大值的索引\n",
        "  - tf.argmax()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tTIrtZ5GnEk",
        "outputId": "2855d1cf-ad36-415d-bdc7-3acc03a9b38f"
      },
      "source": [
        "# p25_reduce_meansum.py\n",
        "import tensorflow as tf\n",
        "\n",
        "x = tf.constant([[1, 2, 3], [2, 2, 3]])\n",
        "print(\"x:\", x)\n",
        "print(\"mean of x:\", tf.reduce_mean(x))  # 求x中所有数的均值\n",
        "print(\"sum of x:\", tf.reduce_sum(x, axis=1))  # 求每一行的和"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x: tf.Tensor(\n",
            "[[1 2 3]\n",
            " [2 2 3]], shape=(2, 3), dtype=int32)\n",
            "mean of x: tf.Tensor(2, shape=(), dtype=int32)\n",
            "sum of x: tf.Tensor([6 7], shape=(2,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6LKBlfrKl0F",
        "outputId": "513060b8-e162-489a-923f-a8c99a0c2f28"
      },
      "source": [
        "# p29_add_subtract_multiply_divide.py\n",
        "import tensorflow as tf\n",
        "\n",
        "a = tf.ones([1, 3])\n",
        "b = tf.fill([1, 3], 3.)\n",
        "print(\"a:\", a)\n",
        "print(\"b:\", b)\n",
        "print(\"a+b:\", tf.add(a, b))\n",
        "print(\"a-b:\", tf.subtract(a, b))\n",
        "print(\"a*b:\", tf.multiply(a, b))\n",
        "print(\"b/a:\", tf.divide(a, b))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a: tf.Tensor([[1. 1. 1.]], shape=(1, 3), dtype=float32)\n",
            "b: tf.Tensor([[3. 3. 3.]], shape=(1, 3), dtype=float32)\n",
            "a+b: tf.Tensor([[4. 4. 4.]], shape=(1, 3), dtype=float32)\n",
            "a-b: tf.Tensor([[-2. -2. -2.]], shape=(1, 3), dtype=float32)\n",
            "a*b: tf.Tensor([[3. 3. 3.]], shape=(1, 3), dtype=float32)\n",
            "b/a: tf.Tensor([[0.33333334 0.33333334 0.33333334]], shape=(1, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkVVuj18Fl7m",
        "outputId": "fcd20844-2d75-4551-e1a0-9a8547b88ca3"
      },
      "source": [
        "# p30_square_pow_sqrt.py\n",
        "import tensorflow as tf\n",
        "\n",
        "a = tf.fill([1, 2], 3.)\n",
        "print(\"a:\", a)\n",
        "print(\"a的3次方:\", tf.pow(a, 3))\n",
        "print(\"a的平方:\", tf.square(a))\n",
        "print(\"a的开方:\", tf.sqrt(a))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a: tf.Tensor([[3. 3.]], shape=(1, 2), dtype=float32)\n",
            "a的3次方: tf.Tensor([[27. 27.]], shape=(1, 2), dtype=float32)\n",
            "a的平方: tf.Tensor([[9. 9.]], shape=(1, 2), dtype=float32)\n",
            "a的开方: tf.Tensor([[1.7320508 1.7320508]], shape=(1, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRa69FRiG8vN",
        "outputId": "059454e5-a68e-44a4-87e2-de2920b8e904"
      },
      "source": [
        "# p31_matmul.py\n",
        "import tensorflow as tf\n",
        "\n",
        "a = tf.ones([3, 2])\n",
        "b = tf.fill([2, 3], 3.)\n",
        "print(\"a:\", a)\n",
        "print(\"b:\", b)\n",
        "\n",
        "print(\"a*b:\", tf.matmul(a, b))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a: tf.Tensor(\n",
            "[[1. 1.]\n",
            " [1. 1.]\n",
            " [1. 1.]], shape=(3, 2), dtype=float32)\n",
            "b: tf.Tensor(\n",
            "[[3. 3. 3.]\n",
            " [3. 3. 3.]], shape=(2, 3), dtype=float32)\n",
            "a*b: tf.Tensor(\n",
            "[[6. 6. 6.]\n",
            " [6. 6. 6.]\n",
            " [6. 6. 6.]], shape=(3, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMv2aGSmIRDS",
        "outputId": "24d1cc62-48aa-4355-d060-3200fe74318a"
      },
      "source": [
        "# p33_from_tensor_slices\n",
        "import tensorflow as tf\n",
        "\n",
        "features = tf.constant([12, 23, 10, 17])\n",
        "labels = tf.constant([0, 1, 1, 0])\n",
        "dataset = tf.data.Dataset.from_tensor_slices((features, labels))    # <--(特征，标签)配对\n",
        "print(dataset)\n",
        "\n",
        "for element in dataset:\n",
        "    print(element)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<TensorSliceDataset shapes: ((), ()), types: (tf.int32, tf.int32)>\n",
            "(<tf.Tensor: shape=(), dtype=int32, numpy=12>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n",
            "(<tf.Tensor: shape=(), dtype=int32, numpy=23>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n",
            "(<tf.Tensor: shape=(), dtype=int32, numpy=10>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n",
            "(<tf.Tensor: shape=(), dtype=int32, numpy=17>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBvA5jrPMUT4",
        "outputId": "6acdbb27-7846-46ff-ec55-bcafc515e45f"
      },
      "source": [
        "# p34_GradientTape.py\n",
        "import tensorflow as tf\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    x = tf.Variable(tf.constant(3.0))\n",
        "    print(\"x:\", x)\n",
        "    y = tf.pow(x, 2)\n",
        "    print(\"y:\", y)\n",
        "grad = tape.gradient(y, x)      #(函数y, 对x求导)\n",
        "print(\"grad:\", grad)\n",
        "\n",
        "# 回到iris case\n",
        "with tf.GradientTape() as tape:\n",
        "    w = tf.Variable(tf.constant(3.0))\n",
        "    loss = tf.pow(w, 2)\n",
        "grad = tape.gradient(loss, w)\n",
        "print(\"grad:\", grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x: <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=3.0>\n",
            "y: tf.Tensor(9.0, shape=(), dtype=float32)\n",
            "grad: tf.Tensor(6.0, shape=(), dtype=float32)\n",
            "grad: tf.Tensor(6.0, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wm4j6QzQi7-",
        "outputId": "56b94fca-11bd-4894-bd81-37c6fb2a6f16"
      },
      "source": [
        "# p35_enumerate.py\n",
        "seq = ['one', 'two', 'three']\n",
        "for i, element in enumerate(seq):\n",
        "    print(i, element)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 one\n",
            "1 two\n",
            "2 three\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QO7GvyEqSg4H",
        "outputId": "424e43e0-8153-43fb-9858-aa916d311097"
      },
      "source": [
        "# p37_one_hot.py\n",
        "import tensorflow as tf\n",
        "\n",
        "classes = 4\n",
        "labels = tf.constant([2, 1, 0])  # 输入的元素值最小为0，最大为2\n",
        "print(\"labels:\", labels)\n",
        "output = tf.one_hot(labels, depth=classes)\n",
        "print(\"result of labels1:\", \"\\n\", output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "labels: tf.Tensor([2 1 0], shape=(3,), dtype=int32)\n",
            "result of labels1: \n",
            " tf.Tensor(\n",
            "[[0. 0. 1. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]], shape=(3, 4), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ranf_I1UVK36",
        "outputId": "40efbe95-cc19-4bc9-f421-286adc8ae327"
      },
      "source": [
        "# p39_softmax.py\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "y = tf.constant([1.01, 2.01, -0.66])\n",
        "y_pro = tf.nn.softmax(y)\n",
        "\n",
        "print(\"After softmax, y_pro is:\", y_pro)  # y_pro 符合概率分布\n",
        "\n",
        "print(\"The sum of y_pro:\", tf.reduce_sum(y_pro))  # 通过softmax后，所有概率加起来和为1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After softmax, y_pro is: tf.Tensor([0.25598174 0.69583046 0.04818781], shape=(3,), dtype=float32)\n",
            "The sum of y_pro: tf.Tensor(1.0, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vb586qDkWKQj",
        "outputId": "de392e4a-2e1d-4096-8165-8caf67974dbc"
      },
      "source": [
        "# p40_assign_sub.py\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "x = tf.Variable(4)    # 赋值并可训练(可以自更新)\n",
        "x.assign_sub(1)   # 自减1\n",
        "print(\"x:\", x)  # 4-1=3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x: <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=3>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ze_OBYzXcFi",
        "outputId": "2b97551e-c4c8-4f82-cd93-90fcd07568be"
      },
      "source": [
        "# p41_argmax.py\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "test = np.array([[1, 2, 3], [2, 3, 4], [5, 4, 3], [8, 7, 2]])\n",
        "print(\"test:\\n\", test)\n",
        "print(\"每一列的最大值的索引：\", tf.argmax(test, axis=0))  # 返回每一列最大值的索引\n",
        "print(\"每一行的最大值的索引\", tf.argmax(test, axis=1))  # 返回每一行最大值的索引"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test:\n",
            " [[1 2 3]\n",
            " [2 3 4]\n",
            " [5 4 3]\n",
            " [8 7 2]]\n",
            "每一列的最大值的索引： tf.Tensor([3 3 1], shape=(3,), dtype=int64)\n",
            "每一行的最大值的索引 tf.Tensor([2 2 0 0], shape=(4,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VdQl4hWYT5W"
      },
      "source": [
        "## Iris 数据集读入\n",
        "\n",
        "### Iris数据集信息: 数据输入特征-所属分类"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOq3vgWqZJR8",
        "outputId": "529f7f25-c45b-4ced-9eb1-45023cef25a2"
      },
      "source": [
        "# p43_datasets.load_iris.py\n",
        "\n",
        "from sklearn import datasets\n",
        "from pandas import DataFrame\n",
        "import pandas as pd\n",
        "\n",
        "x_data = datasets.load_iris().data  # .data返回iris数据集所有输入特征\n",
        "y_data = datasets.load_iris().target  # .target返回iris数据集所有标签\n",
        "print(\"x_data from datasets: \\n\", x_data)\n",
        "print(\"y_data from datasets: \\n\", y_data)\n",
        "\n",
        "x_data = DataFrame(x_data, columns=['花萼长度', '花萼宽度', '花瓣长度', '花瓣宽度']) # 为表格增加行索引（左侧）和列标签（上方）\n",
        "pd.set_option('display.unicode.east_asian_width', True)  # 设置列名对齐\n",
        "print(\"x_data add index and colnames: \\n\", x_data)\n",
        "\n",
        "x_data['类别'] = y_data  # 新加一列，列标签为‘类别’，数据为y_data\n",
        "print(\"x_data add a column: \\n\", x_data)\n",
        "\n",
        "#类型维度不确定时，建议用print函数打印出来确认效果"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_data from datasets: \n",
            " [[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.  3.6 1.4 0.2]\n",
            " [5.4 3.9 1.7 0.4]\n",
            " [4.6 3.4 1.4 0.3]\n",
            " [5.  3.4 1.5 0.2]\n",
            " [4.4 2.9 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [5.4 3.7 1.5 0.2]\n",
            " [4.8 3.4 1.6 0.2]\n",
            " [4.8 3.  1.4 0.1]\n",
            " [4.3 3.  1.1 0.1]\n",
            " [5.8 4.  1.2 0.2]\n",
            " [5.7 4.4 1.5 0.4]\n",
            " [5.4 3.9 1.3 0.4]\n",
            " [5.1 3.5 1.4 0.3]\n",
            " [5.7 3.8 1.7 0.3]\n",
            " [5.1 3.8 1.5 0.3]\n",
            " [5.4 3.4 1.7 0.2]\n",
            " [5.1 3.7 1.5 0.4]\n",
            " [4.6 3.6 1.  0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.8 3.4 1.9 0.2]\n",
            " [5.  3.  1.6 0.2]\n",
            " [5.  3.4 1.6 0.4]\n",
            " [5.2 3.5 1.5 0.2]\n",
            " [5.2 3.4 1.4 0.2]\n",
            " [4.7 3.2 1.6 0.2]\n",
            " [4.8 3.1 1.6 0.2]\n",
            " [5.4 3.4 1.5 0.4]\n",
            " [5.2 4.1 1.5 0.1]\n",
            " [5.5 4.2 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.2]\n",
            " [5.  3.2 1.2 0.2]\n",
            " [5.5 3.5 1.3 0.2]\n",
            " [4.9 3.6 1.4 0.1]\n",
            " [4.4 3.  1.3 0.2]\n",
            " [5.1 3.4 1.5 0.2]\n",
            " [5.  3.5 1.3 0.3]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [4.4 3.2 1.3 0.2]\n",
            " [5.  3.5 1.6 0.6]\n",
            " [5.1 3.8 1.9 0.4]\n",
            " [4.8 3.  1.4 0.3]\n",
            " [5.1 3.8 1.6 0.2]\n",
            " [4.6 3.2 1.4 0.2]\n",
            " [5.3 3.7 1.5 0.2]\n",
            " [5.  3.3 1.4 0.2]\n",
            " [7.  3.2 4.7 1.4]\n",
            " [6.4 3.2 4.5 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [5.5 2.3 4.  1.3]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [5.7 2.8 4.5 1.3]\n",
            " [6.3 3.3 4.7 1.6]\n",
            " [4.9 2.4 3.3 1. ]\n",
            " [6.6 2.9 4.6 1.3]\n",
            " [5.2 2.7 3.9 1.4]\n",
            " [5.  2.  3.5 1. ]\n",
            " [5.9 3.  4.2 1.5]\n",
            " [6.  2.2 4.  1. ]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [5.6 2.9 3.6 1.3]\n",
            " [6.7 3.1 4.4 1.4]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.8 2.7 4.1 1. ]\n",
            " [6.2 2.2 4.5 1.5]\n",
            " [5.6 2.5 3.9 1.1]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [6.1 2.8 4.  1.3]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.8 4.7 1.2]\n",
            " [6.4 2.9 4.3 1.3]\n",
            " [6.6 3.  4.4 1.4]\n",
            " [6.8 2.8 4.8 1.4]\n",
            " [6.7 3.  5.  1.7]\n",
            " [6.  2.9 4.5 1.5]\n",
            " [5.7 2.6 3.5 1. ]\n",
            " [5.5 2.4 3.8 1.1]\n",
            " [5.5 2.4 3.7 1. ]\n",
            " [5.8 2.7 3.9 1.2]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.  3.4 4.5 1.6]\n",
            " [6.7 3.1 4.7 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [5.6 3.  4.1 1.3]\n",
            " [5.5 2.5 4.  1.3]\n",
            " [5.5 2.6 4.4 1.2]\n",
            " [6.1 3.  4.6 1.4]\n",
            " [5.8 2.6 4.  1.2]\n",
            " [5.  2.3 3.3 1. ]\n",
            " [5.6 2.7 4.2 1.3]\n",
            " [5.7 3.  4.2 1.2]\n",
            " [5.7 2.9 4.2 1.3]\n",
            " [6.2 2.9 4.3 1.3]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [5.7 2.8 4.1 1.3]\n",
            " [6.3 3.3 6.  2.5]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [7.1 3.  5.9 2.1]\n",
            " [6.3 2.9 5.6 1.8]\n",
            " [6.5 3.  5.8 2.2]\n",
            " [7.6 3.  6.6 2.1]\n",
            " [4.9 2.5 4.5 1.7]\n",
            " [7.3 2.9 6.3 1.8]\n",
            " [6.7 2.5 5.8 1.8]\n",
            " [7.2 3.6 6.1 2.5]\n",
            " [6.5 3.2 5.1 2. ]\n",
            " [6.4 2.7 5.3 1.9]\n",
            " [6.8 3.  5.5 2.1]\n",
            " [5.7 2.5 5.  2. ]\n",
            " [5.8 2.8 5.1 2.4]\n",
            " [6.4 3.2 5.3 2.3]\n",
            " [6.5 3.  5.5 1.8]\n",
            " [7.7 3.8 6.7 2.2]\n",
            " [7.7 2.6 6.9 2.3]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.9 3.2 5.7 2.3]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [7.7 2.8 6.7 2. ]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.7 3.3 5.7 2.1]\n",
            " [7.2 3.2 6.  1.8]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.1 3.  4.9 1.8]\n",
            " [6.4 2.8 5.6 2.1]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [7.4 2.8 6.1 1.9]\n",
            " [7.9 3.8 6.4 2. ]\n",
            " [6.4 2.8 5.6 2.2]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.1 2.6 5.6 1.4]\n",
            " [7.7 3.  6.1 2.3]\n",
            " [6.3 3.4 5.6 2.4]\n",
            " [6.4 3.1 5.5 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.9 3.1 5.4 2.1]\n",
            " [6.7 3.1 5.6 2.4]\n",
            " [6.9 3.1 5.1 2.3]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [6.8 3.2 5.9 2.3]\n",
            " [6.7 3.3 5.7 2.5]\n",
            " [6.7 3.  5.2 2.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [6.2 3.4 5.4 2.3]\n",
            " [5.9 3.  5.1 1.8]]\n",
            "y_data from datasets: \n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2]\n",
            "x_data add index and colnames: \n",
            "      花萼长度  花萼宽度  花瓣长度  花瓣宽度\n",
            "0         5.1       3.5       1.4       0.2\n",
            "1         4.9       3.0       1.4       0.2\n",
            "2         4.7       3.2       1.3       0.2\n",
            "3         4.6       3.1       1.5       0.2\n",
            "4         5.0       3.6       1.4       0.2\n",
            "..        ...       ...       ...       ...\n",
            "145       6.7       3.0       5.2       2.3\n",
            "146       6.3       2.5       5.0       1.9\n",
            "147       6.5       3.0       5.2       2.0\n",
            "148       6.2       3.4       5.4       2.3\n",
            "149       5.9       3.0       5.1       1.8\n",
            "\n",
            "[150 rows x 4 columns]\n",
            "x_data add a column: \n",
            "      花萼长度  花萼宽度  花瓣长度  花瓣宽度  类别\n",
            "0         5.1       3.5       1.4       0.2     0\n",
            "1         4.9       3.0       1.4       0.2     0\n",
            "2         4.7       3.2       1.3       0.2     0\n",
            "3         4.6       3.1       1.5       0.2     0\n",
            "4         5.0       3.6       1.4       0.2     0\n",
            "..        ...       ...       ...       ...   ...\n",
            "145       6.7       3.0       5.2       2.3     2\n",
            "146       6.3       2.5       5.0       1.9     2\n",
            "147       6.5       3.0       5.2       2.0     2\n",
            "148       6.2       3.4       5.4       2.3     2\n",
            "149       5.9       3.0       5.1       1.8     2\n",
            "\n",
            "[150 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFRazWTJbnV6"
      },
      "source": [
        "## 神经网络实现iris分类"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3qwvFZm9gjRG",
        "outputId": "69d7a8aa-bcc9-4ec6-cb9c-ba966d73c77c"
      },
      "source": [
        "# p45_iris.py\n",
        "# -*- coding: UTF-8 -*-\n",
        "# 利用鸢尾花数据集，实现前向传播、反向传播，可视化loss曲线\n",
        "\n",
        "# 导入所需模块\n",
        "import tensorflow as tf\n",
        "from sklearn import datasets\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 导入数据，分别为输入特征和标签\n",
        "x_data = datasets.load_iris().data\n",
        "y_data = datasets.load_iris().target\n",
        "\n",
        "# 随机打乱数据（因为原始数据是顺序的，顺序不打乱会影响准确率）\n",
        "# seed: 随机数种子，是一个整数，当设置之后，每次生成的随机数都一样（为方便教学，以保每位同学结果一致）\n",
        "np.random.seed(116)  # 使用相同的seed，保证输入特征和标签一一对应\n",
        "np.random.shuffle(x_data)\n",
        "np.random.seed(116)\n",
        "np.random.shuffle(y_data)\n",
        "tf.random.set_seed(116)\n",
        "\n",
        "# 将打乱后的数据集分割为永不相见的训练集和测试集，训练集为前120行，测试集为后30行\n",
        "x_train = x_data[:-30]\n",
        "y_train = y_data[:-30]\n",
        "x_test = x_data[-30:]\n",
        "y_test = y_data[-30:]\n",
        "\n",
        "# 转换x的数据类型，否则后面矩阵相乘时会因数据类型不一致报错\n",
        "x_train = tf.cast(x_train, tf.float32)\n",
        "x_test = tf.cast(x_test, tf.float32)\n",
        "\n",
        "# from_tensor_slices函数使输入特征和标签值一一对应。（把数据集分批次，每个批次batch组数据）\n",
        "train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
        "test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n",
        "\n",
        "# 生成神经网络的参数，4个输入特征故，输入层为4个输入节点；因为3分类，故输出层为3个神经元\n",
        "# 用tf.Variable()标记参数可训练\n",
        "# 使用seed使每次生成的随机数相同（方便教学，使大家结果都一致，在现实使用时不写seed）\n",
        "w1 = tf.Variable(tf.random.truncated_normal([4, 3], stddev=0.1, seed=1))\n",
        "b1 = tf.Variable(tf.random.truncated_normal([3], stddev=0.1, seed=1))\n",
        "\n",
        "lr = 0.1  # 学习率为0.1\n",
        "train_loss_results = []  # 将每轮的loss记录在此列表中，为后续画loss曲线提供数据\n",
        "test_acc = []  # 将每轮的acc记录在此列表中，为后续画acc曲线提供数据\n",
        "epoch = 500  # 循环500轮\n",
        "loss_all = 0  # 每轮分4个step，loss_all记录四个step生成的4个loss的和\n",
        "\n",
        "# 训练部分\n",
        "for epoch in range(epoch):  #数据集级别的循环，每个epoch循环一次数据集\n",
        "    for step, (x_train, y_train) in enumerate(train_db):  #batch级别的循环 ，每个step循环一个batch\n",
        "        with tf.GradientTape() as tape:  # with结构记录梯度信息\n",
        "            y = tf.matmul(x_train, w1) + b1  # 神经网络乘加运算\n",
        "            y = tf.nn.softmax(y)  # 使输出y符合概率分布（此操作后与独热码同量级，可相减求loss）\n",
        "            y_ = tf.one_hot(y_train, depth=3)  # 将标签值转换为独热码格式，方便计算loss和accuracy\n",
        "            loss = tf.reduce_mean(tf.square(y_ - y))  # 采用均方误差损失函数mse = mean(sum(y-out)^2)\n",
        "            loss_all += loss.numpy()  # 将每个step计算出的loss累加，为后续求loss平均值提供数据，这样计算的loss更准确\n",
        "        # 计算loss对各个参数的梯度(求偏导)\n",
        "        grads = tape.gradient(loss, [w1, b1])\n",
        "\n",
        "        # 实现梯度更新 w1 = w1 - lr * w1_grad    b = b - lr * b_grad\n",
        "        w1.assign_sub(lr * grads[0])  # 参数w1自更新\n",
        "        b1.assign_sub(lr * grads[1])  # 参数b自更新\n",
        "\n",
        "    # 每个epoch，打印loss信息\n",
        "    print(\"Epoch {}, loss: {}\".format(epoch, loss_all/4))\n",
        "    train_loss_results.append(loss_all / 4)  # 将4个step的loss求平均记录在此变量中\n",
        "    loss_all = 0  # loss_all归零，为记录下一个epoch的loss做准备\n",
        "\n",
        "    # 测试部分\n",
        "    # total_correct为预测对的样本个数, total_number为测试的总样本数，将这两个变量都初始化为0\n",
        "    total_correct, total_number = 0, 0\n",
        "    for x_test, y_test in test_db:\n",
        "        # 使用更新后的参数进行预测\n",
        "        y = tf.matmul(x_test, w1) + b1    #计算前向传播的预测结果\n",
        "        y = tf.nn.softmax(y)        # 变为概率分布\n",
        "        pred = tf.argmax(y, axis=1)  # 返回y中最大值的索引，即预测的分类\n",
        "        # 将pred转换为y_test的数据类型\n",
        "        pred = tf.cast(pred, dtype=y_test.dtype)\n",
        "        # 若分类正确，则correct=1，否则为0，将bool型的结果转换为int型\n",
        "        correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32)\n",
        "        # 将每个batch的correct数加起来\n",
        "        correct = tf.reduce_sum(correct)\n",
        "        # 将所有batch中的correct数加起来\n",
        "        total_correct += int(correct)\n",
        "        # total_number为测试的总样本数，也就是x_test的行数，shape[0]返回变量的行数\n",
        "        total_number += x_test.shape[0]\n",
        "    # 总的准确率等于total_correct/total_number\n",
        "    acc = total_correct / total_number\n",
        "    test_acc.append(acc)\n",
        "    print(\"Test_acc:\", acc)\n",
        "    print(\"--------------------------\")\n",
        "\n",
        "# 绘制 loss 曲线\n",
        "plt.title('Loss Function Curve')  # 图片标题\n",
        "plt.xlabel('Epoch')  # x轴变量名称\n",
        "plt.ylabel('Loss')  # y轴变量名称\n",
        "plt.plot(train_loss_results, label=\"$Loss$\")  # 逐点画出trian_loss_results值并连线，连线图标是Loss\n",
        "plt.legend()  # 画出曲线图标\n",
        "plt.show()  # 画出图像\n",
        "\n",
        "# 绘制 Accuracy 曲线\n",
        "plt.title('Acc Curve')  # 图片标题\n",
        "plt.xlabel('Epoch')  # x轴变量名称\n",
        "plt.ylabel('Acc')  # y轴变量名称\n",
        "plt.plot(test_acc, label=\"$Accuracy$\")  # 逐点画出test_acc值并连线，连线图标是Accuracy\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, loss: 0.2821310982108116\n",
            "Test_acc: 0.16666666666666666\n",
            "--------------------------\n",
            "Epoch 1, loss: 0.25459614023566246\n",
            "Test_acc: 0.16666666666666666\n",
            "--------------------------\n",
            "Epoch 2, loss: 0.22570250928401947\n",
            "Test_acc: 0.16666666666666666\n",
            "--------------------------\n",
            "Epoch 3, loss: 0.21028399094939232\n",
            "Test_acc: 0.16666666666666666\n",
            "--------------------------\n",
            "Epoch 4, loss: 0.19942265003919601\n",
            "Test_acc: 0.16666666666666666\n",
            "--------------------------\n",
            "Epoch 5, loss: 0.18873637914657593\n",
            "Test_acc: 0.5\n",
            "--------------------------\n",
            "Epoch 6, loss: 0.17851299792528152\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 7, loss: 0.16922875866293907\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 8, loss: 0.16107673197984695\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 9, loss: 0.15404683724045753\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 10, loss: 0.14802725985646248\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 11, loss: 0.14287303388118744\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 12, loss: 0.1384414117783308\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 13, loss: 0.13460607454180717\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 14, loss: 0.1312607266008854\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 15, loss: 0.12831821478903294\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 16, loss: 0.12570795975625515\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 17, loss: 0.12337299063801765\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 18, loss: 0.12126746587455273\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 19, loss: 0.11935433372855186\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 20, loss: 0.11760355345904827\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 21, loss: 0.11599067784845829\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 22, loss: 0.11449568346142769\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 23, loss: 0.11310207471251488\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 24, loss: 0.11179621890187263\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 25, loss: 0.11056671850383282\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 26, loss: 0.10940407775342464\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 27, loss: 0.10830028168857098\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 28, loss: 0.10724855773150921\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 29, loss: 0.10624313540756702\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 30, loss: 0.1052790954709053\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 31, loss: 0.10435222089290619\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 32, loss: 0.10345886647701263\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 33, loss: 0.1025958750396967\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 34, loss: 0.10176053084433079\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 35, loss: 0.10095042362809181\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 36, loss: 0.10016347467899323\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 37, loss: 0.09939785115420818\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 38, loss: 0.098651934415102\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 39, loss: 0.09792428836226463\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 40, loss: 0.09721364825963974\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 41, loss: 0.09651889465749264\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 42, loss: 0.09583901800215244\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 43, loss: 0.09517310746014118\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 44, loss: 0.09452036581933498\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 45, loss: 0.09388007409870625\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 46, loss: 0.09325156360864639\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 47, loss: 0.09263425506651402\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 48, loss: 0.09202759712934494\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 49, loss: 0.09143111854791641\n",
            "Test_acc: 0.5333333333333333\n",
            "--------------------------\n",
            "Epoch 50, loss: 0.09084435924887657\n",
            "Test_acc: 0.5666666666666667\n",
            "--------------------------\n",
            "Epoch 51, loss: 0.09026693738996983\n",
            "Test_acc: 0.5666666666666667\n",
            "--------------------------\n",
            "Epoch 52, loss: 0.08969846740365028\n",
            "Test_acc: 0.5666666666666667\n",
            "--------------------------\n",
            "Epoch 53, loss: 0.08913860842585564\n",
            "Test_acc: 0.6\n",
            "--------------------------\n",
            "Epoch 54, loss: 0.08858705498278141\n",
            "Test_acc: 0.6\n",
            "--------------------------\n",
            "Epoch 55, loss: 0.08804351836442947\n",
            "Test_acc: 0.6\n",
            "--------------------------\n",
            "Epoch 56, loss: 0.08750772848725319\n",
            "Test_acc: 0.6\n",
            "--------------------------\n",
            "Epoch 57, loss: 0.08697944693267345\n",
            "Test_acc: 0.6\n",
            "--------------------------\n",
            "Epoch 58, loss: 0.08645843341946602\n",
            "Test_acc: 0.6\n",
            "--------------------------\n",
            "Epoch 59, loss: 0.08594449236989021\n",
            "Test_acc: 0.6\n",
            "--------------------------\n",
            "Epoch 60, loss: 0.08543741330504417\n",
            "Test_acc: 0.6\n",
            "--------------------------\n",
            "Epoch 61, loss: 0.08493702113628387\n",
            "Test_acc: 0.6\n",
            "--------------------------\n",
            "Epoch 62, loss: 0.08444313518702984\n",
            "Test_acc: 0.6333333333333333\n",
            "--------------------------\n",
            "Epoch 63, loss: 0.08395559713244438\n",
            "Test_acc: 0.6333333333333333\n",
            "--------------------------\n",
            "Epoch 64, loss: 0.08347426541149616\n",
            "Test_acc: 0.6333333333333333\n",
            "--------------------------\n",
            "Epoch 65, loss: 0.08299898356199265\n",
            "Test_acc: 0.6333333333333333\n",
            "--------------------------\n",
            "Epoch 66, loss: 0.08252961747348309\n",
            "Test_acc: 0.6333333333333333\n",
            "--------------------------\n",
            "Epoch 67, loss: 0.08206604234874249\n",
            "Test_acc: 0.6333333333333333\n",
            "--------------------------\n",
            "Epoch 68, loss: 0.08160813339054585\n",
            "Test_acc: 0.6333333333333333\n",
            "--------------------------\n",
            "Epoch 69, loss: 0.08115578815340996\n",
            "Test_acc: 0.6333333333333333\n",
            "--------------------------\n",
            "Epoch 70, loss: 0.08070887438952923\n",
            "Test_acc: 0.6333333333333333\n",
            "--------------------------\n",
            "Epoch 71, loss: 0.08026731759309769\n",
            "Test_acc: 0.6333333333333333\n",
            "--------------------------\n",
            "Epoch 72, loss: 0.07983099296689034\n",
            "Test_acc: 0.6666666666666666\n",
            "--------------------------\n",
            "Epoch 73, loss: 0.07939982041716576\n",
            "Test_acc: 0.6666666666666666\n",
            "--------------------------\n",
            "Epoch 74, loss: 0.07897370308637619\n",
            "Test_acc: 0.6666666666666666\n",
            "--------------------------\n",
            "Epoch 75, loss: 0.07855254970490932\n",
            "Test_acc: 0.7\n",
            "--------------------------\n",
            "Epoch 76, loss: 0.07813628017902374\n",
            "Test_acc: 0.7\n",
            "--------------------------\n",
            "Epoch 77, loss: 0.07772481627762318\n",
            "Test_acc: 0.7\n",
            "--------------------------\n",
            "Epoch 78, loss: 0.07731806859374046\n",
            "Test_acc: 0.7\n",
            "--------------------------\n",
            "Epoch 79, loss: 0.07691597938537598\n",
            "Test_acc: 0.7\n",
            "--------------------------\n",
            "Epoch 80, loss: 0.07651844993233681\n",
            "Test_acc: 0.7\n",
            "--------------------------\n",
            "Epoch 81, loss: 0.07612544856965542\n",
            "Test_acc: 0.7333333333333333\n",
            "--------------------------\n",
            "Epoch 82, loss: 0.075736865401268\n",
            "Test_acc: 0.7333333333333333\n",
            "--------------------------\n",
            "Epoch 83, loss: 0.07535266131162643\n",
            "Test_acc: 0.7333333333333333\n",
            "--------------------------\n",
            "Epoch 84, loss: 0.07497274875640869\n",
            "Test_acc: 0.7333333333333333\n",
            "--------------------------\n",
            "Epoch 85, loss: 0.0745970867574215\n",
            "Test_acc: 0.7666666666666667\n",
            "--------------------------\n",
            "Epoch 86, loss: 0.07422560174018145\n",
            "Test_acc: 0.7666666666666667\n",
            "--------------------------\n",
            "Epoch 87, loss: 0.07385823410004377\n",
            "Test_acc: 0.7666666666666667\n",
            "--------------------------\n",
            "Epoch 88, loss: 0.07349492609500885\n",
            "Test_acc: 0.7666666666666667\n",
            "--------------------------\n",
            "Epoch 89, loss: 0.0731356255710125\n",
            "Test_acc: 0.7666666666666667\n",
            "--------------------------\n",
            "Epoch 90, loss: 0.07278026919811964\n",
            "Test_acc: 0.7666666666666667\n",
            "--------------------------\n",
            "Epoch 91, loss: 0.0724288085475564\n",
            "Test_acc: 0.7666666666666667\n",
            "--------------------------\n",
            "Epoch 92, loss: 0.0720811877399683\n",
            "Test_acc: 0.7666666666666667\n",
            "--------------------------\n",
            "Epoch 93, loss: 0.07173734437674284\n",
            "Test_acc: 0.8\n",
            "--------------------------\n",
            "Epoch 94, loss: 0.07139724120497704\n",
            "Test_acc: 0.8\n",
            "--------------------------\n",
            "Epoch 95, loss: 0.07106082327663898\n",
            "Test_acc: 0.8\n",
            "--------------------------\n",
            "Epoch 96, loss: 0.07072803657501936\n",
            "Test_acc: 0.8\n",
            "--------------------------\n",
            "Epoch 97, loss: 0.07039884105324745\n",
            "Test_acc: 0.8\n",
            "--------------------------\n",
            "Epoch 98, loss: 0.07007318269461393\n",
            "Test_acc: 0.8333333333333334\n",
            "--------------------------\n",
            "Epoch 99, loss: 0.06975101679563522\n",
            "Test_acc: 0.8666666666666667\n",
            "--------------------------\n",
            "Epoch 100, loss: 0.06943229679018259\n",
            "Test_acc: 0.8666666666666667\n",
            "--------------------------\n",
            "Epoch 101, loss: 0.06911697331815958\n",
            "Test_acc: 0.8666666666666667\n",
            "--------------------------\n",
            "Epoch 102, loss: 0.06880501005798578\n",
            "Test_acc: 0.8666666666666667\n",
            "--------------------------\n",
            "Epoch 103, loss: 0.0684963557869196\n",
            "Test_acc: 0.8666666666666667\n",
            "--------------------------\n",
            "Epoch 104, loss: 0.06819096300750971\n",
            "Test_acc: 0.8666666666666667\n",
            "--------------------------\n",
            "Epoch 105, loss: 0.06788880191743374\n",
            "Test_acc: 0.8666666666666667\n",
            "--------------------------\n",
            "Epoch 106, loss: 0.06758982222527266\n",
            "Test_acc: 0.8666666666666667\n",
            "--------------------------\n",
            "Epoch 107, loss: 0.0672939857468009\n",
            "Test_acc: 0.9\n",
            "--------------------------\n",
            "Epoch 108, loss: 0.06700124684721231\n",
            "Test_acc: 0.9\n",
            "--------------------------\n",
            "Epoch 109, loss: 0.06671156641095877\n",
            "Test_acc: 0.9\n",
            "--------------------------\n",
            "Epoch 110, loss: 0.06642491556704044\n",
            "Test_acc: 0.9\n",
            "--------------------------\n",
            "Epoch 111, loss: 0.06614124123007059\n",
            "Test_acc: 0.9\n",
            "--------------------------\n",
            "Epoch 112, loss: 0.06586051359772682\n",
            "Test_acc: 0.9\n",
            "--------------------------\n",
            "Epoch 113, loss: 0.06558268796652555\n",
            "Test_acc: 0.9\n",
            "--------------------------\n",
            "Epoch 114, loss: 0.06530772987753153\n",
            "Test_acc: 0.9\n",
            "--------------------------\n",
            "Epoch 115, loss: 0.06503561232239008\n",
            "Test_acc: 0.9\n",
            "--------------------------\n",
            "Epoch 116, loss: 0.06476627569645643\n",
            "Test_acc: 0.9\n",
            "--------------------------\n",
            "Epoch 117, loss: 0.06449970323592424\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 118, loss: 0.06423585955053568\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 119, loss: 0.0639747017994523\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 120, loss: 0.06371619831770658\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 121, loss: 0.06346031371504068\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 122, loss: 0.06320701539516449\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 123, loss: 0.06295627355575562\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 124, loss: 0.06270804908126593\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 125, loss: 0.06246231496334076\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 126, loss: 0.06221903674304485\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 127, loss: 0.061978189274668694\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 128, loss: 0.06173973251134157\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 129, loss: 0.06150364130735397\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 130, loss: 0.06126988772302866\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 131, loss: 0.0610384326428175\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 132, loss: 0.06080926116555929\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 133, loss: 0.060582331381738186\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 134, loss: 0.06035762559622526\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 135, loss: 0.06013511028140783\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 136, loss: 0.05991474911570549\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 137, loss: 0.05969652533531189\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 138, loss: 0.059480415657162666\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 139, loss: 0.05926638841629028\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 140, loss: 0.05905440915375948\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 141, loss: 0.05884446296840906\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 142, loss: 0.05863652378320694\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 143, loss: 0.058430569246411324\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 144, loss: 0.05822655279189348\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 145, loss: 0.05802448280155659\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 146, loss: 0.05782431177794933\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 147, loss: 0.05762602761387825\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 148, loss: 0.0574295949190855\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 149, loss: 0.05723499599844217\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 150, loss: 0.05704221688210964\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 151, loss: 0.056851224042475224\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 152, loss: 0.05666199326515198\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 153, loss: 0.05647451803088188\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 154, loss: 0.056288765743374825\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 155, loss: 0.05610471311956644\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 156, loss: 0.05592234339565039\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 157, loss: 0.05574163515120745\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 158, loss: 0.05556256137788296\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 159, loss: 0.05538511835038662\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 160, loss: 0.05520927347242832\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 161, loss: 0.05503500904887915\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 162, loss: 0.0548623101785779\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 163, loss: 0.054691143333911896\n",
            "Test_acc: 0.9333333333333333\n",
            "--------------------------\n",
            "Epoch 164, loss: 0.05452151596546173\n",
            "Test_acc: 0.9666666666666667\n",
            "--------------------------\n",
            "Epoch 165, loss: 0.054353379644453526\n",
            "Test_acc: 0.9666666666666667\n",
            "--------------------------\n",
            "Epoch 166, loss: 0.05418673437088728\n",
            "Test_acc: 0.9666666666666667\n",
            "--------------------------\n",
            "Epoch 167, loss: 0.05402155872434378\n",
            "Test_acc: 0.9666666666666667\n",
            "--------------------------\n",
            "Epoch 168, loss: 0.05385783687233925\n",
            "Test_acc: 0.9666666666666667\n",
            "--------------------------\n",
            "Epoch 169, loss: 0.053695556707680225\n",
            "Test_acc: 0.9666666666666667\n",
            "--------------------------\n",
            "Epoch 170, loss: 0.05353467911481857\n",
            "Test_acc: 0.9666666666666667\n",
            "--------------------------\n",
            "Epoch 171, loss: 0.053375206887722015\n",
            "Test_acc: 0.9666666666666667\n",
            "--------------------------\n",
            "Epoch 172, loss: 0.05321711953729391\n",
            "Test_acc: 0.9666666666666667\n",
            "--------------------------\n",
            "Epoch 173, loss: 0.053060395643115044\n",
            "Test_acc: 0.9666666666666667\n",
            "--------------------------\n",
            "Epoch 174, loss: 0.05290502496063709\n",
            "Test_acc: 0.9666666666666667\n",
            "--------------------------\n",
            "Epoch 175, loss: 0.05275098513811827\n",
            "Test_acc: 0.9666666666666667\n",
            "--------------------------\n",
            "Epoch 176, loss: 0.05259825848042965\n",
            "Test_acc: 0.9666666666666667\n",
            "--------------------------\n",
            "Epoch 177, loss: 0.05244683939963579\n",
            "Test_acc: 0.9666666666666667\n",
            "--------------------------\n",
            "Epoch 178, loss: 0.05229670833796263\n",
            "Test_acc: 0.9666666666666667\n",
            "--------------------------\n",
            "Epoch 179, loss: 0.05214784760028124\n",
            "Test_acc: 0.9666666666666667\n",
            "--------------------------\n",
            "Epoch 180, loss: 0.052000248804688454\n",
            "Test_acc: 0.9666666666666667\n",
            "--------------------------\n",
            "Epoch 181, loss: 0.051853885874152184\n",
            "Test_acc: 0.9666666666666667\n",
            "--------------------------\n",
            "Epoch 182, loss: 0.05170875322073698\n",
            "Test_acc: 0.9666666666666667\n",
            "--------------------------\n",
            "Epoch 183, loss: 0.05156483594328165\n",
            "Test_acc: 0.9666666666666667\n",
            "--------------------------\n",
            "Epoch 184, loss: 0.0514221154153347\n",
            "Test_acc: 0.9666666666666667\n",
            "--------------------------\n",
            "Epoch 185, loss: 0.051280584186315536\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 186, loss: 0.05114021711051464\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 187, loss: 0.05100101325660944\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 188, loss: 0.050862944684922695\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 189, loss: 0.05072600953280926\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 190, loss: 0.05059020034968853\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 191, loss: 0.05045549105852842\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 192, loss: 0.05032187607139349\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 193, loss: 0.050189344212412834\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 194, loss: 0.05005786940455437\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 195, loss: 0.049927459098398685\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 196, loss: 0.04979808907955885\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 197, loss: 0.049669744446873665\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 198, loss: 0.049542427994310856\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 199, loss: 0.04941611923277378\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 200, loss: 0.04929081164300442\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 201, loss: 0.0491664744913578\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 202, loss: 0.049043125472962856\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 203, loss: 0.048920733854174614\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 204, loss: 0.0487992987036705\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 205, loss: 0.04867880791425705\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 206, loss: 0.04855924379080534\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 207, loss: 0.04844060353934765\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 208, loss: 0.048322875052690506\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 209, loss: 0.04820605181157589\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 210, loss: 0.04809011612087488\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 211, loss: 0.04797506146132946\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 212, loss: 0.0478608813136816\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 213, loss: 0.04774756357073784\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 214, loss: 0.047635097056627274\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 215, loss: 0.04752347245812416\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 216, loss: 0.0474126860499382\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 217, loss: 0.047302727587521076\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 218, loss: 0.04719358589500189\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 219, loss: 0.04708525165915489\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 220, loss: 0.04697771091014147\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 221, loss: 0.04687096830457449\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 222, loss: 0.04676501080393791\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 223, loss: 0.04665982723236084\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 224, loss: 0.04655539896339178\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 225, loss: 0.046451738104224205\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 226, loss: 0.046348835341632366\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 227, loss: 0.04624665342271328\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 228, loss: 0.046145214699208736\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 229, loss: 0.046044510789215565\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 230, loss: 0.045944519340991974\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 231, loss: 0.04584524221718311\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 232, loss: 0.045746663585305214\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 233, loss: 0.04564878437668085\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 234, loss: 0.045551604591310024\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 235, loss: 0.045455098152160645\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 236, loss: 0.04535927437245846\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 237, loss: 0.045264110900461674\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 238, loss: 0.045169608667492867\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 239, loss: 0.04507576674222946\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 240, loss: 0.04498257301747799\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 241, loss: 0.044890021905303\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 242, loss: 0.0447981134057045\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 243, loss: 0.04470681585371494\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 244, loss: 0.04461615812033415\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 245, loss: 0.04452610854059458\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 246, loss: 0.044436678290367126\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 247, loss: 0.04434784408658743\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 248, loss: 0.04425961244851351\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 249, loss: 0.044171975925564766\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 250, loss: 0.04408491309732199\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 251, loss: 0.04399844352155924\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 252, loss: 0.043912545777857304\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 253, loss: 0.04382721334695816\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 254, loss: 0.04374244902282953\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 255, loss: 0.04365824814885855\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 256, loss: 0.043574594892561436\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 257, loss: 0.0434914892539382\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 258, loss: 0.04340892657637596\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 259, loss: 0.04332689940929413\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 260, loss: 0.0432454077526927\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 261, loss: 0.043164439499378204\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 262, loss: 0.04308400023728609\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 263, loss: 0.043004073202610016\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 264, loss: 0.04292465187609196\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 265, loss: 0.042845748364925385\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 266, loss: 0.04276734683662653\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 267, loss: 0.0426894361153245\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 268, loss: 0.04261201433837414\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 269, loss: 0.042535096406936646\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 270, loss: 0.042458648793399334\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 271, loss: 0.042382681742310524\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 272, loss: 0.04230719804763794\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 273, loss: 0.04223218001425266\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 274, loss: 0.042157627642154694\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 275, loss: 0.04208353813737631\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 276, loss: 0.042009901255369186\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 277, loss: 0.04193672351539135\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 278, loss: 0.04186399281024933\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 279, loss: 0.041791705414652824\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 280, loss: 0.04171985574066639\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 281, loss: 0.0416484484449029\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 282, loss: 0.0415774742141366\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 283, loss: 0.04150693118572235\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 284, loss: 0.04143680725246668\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 285, loss: 0.04136710520833731\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 286, loss: 0.04129782412201166\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 287, loss: 0.041228956542909145\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 288, loss: 0.041160495951771736\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 289, loss: 0.04109245166182518\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 290, loss: 0.041024803183972836\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 291, loss: 0.0409575579687953\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 292, loss: 0.040890698321163654\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 293, loss: 0.04082423634827137\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 294, loss: 0.040758173912763596\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 295, loss: 0.04069248586893082\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 296, loss: 0.040627180598676205\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 297, loss: 0.04056225158274174\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 298, loss: 0.04049770347774029\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 299, loss: 0.040433524176478386\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 300, loss: 0.040369716472923756\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 301, loss: 0.04030627757310867\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 302, loss: 0.04024319630116224\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 303, loss: 0.04018047358840704\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 304, loss: 0.04011811316013336\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 305, loss: 0.04005609964951873\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 306, loss: 0.0399944419041276\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 307, loss: 0.03993312967941165\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 308, loss: 0.03987216204404831\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 309, loss: 0.039811535738408566\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 310, loss: 0.03975125262513757\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 311, loss: 0.03969130339100957\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 312, loss: 0.03963168291375041\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 313, loss: 0.0395723944529891\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 314, loss: 0.03951343335211277\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 315, loss: 0.039454799611121416\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 316, loss: 0.03939648764207959\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 317, loss: 0.0393384899944067\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 318, loss: 0.03928081365302205\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 319, loss: 0.03922345116734505\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 320, loss: 0.03916640346869826\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 321, loss: 0.03910966170951724\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 322, loss: 0.039053224958479404\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 323, loss: 0.038997096475213766\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 324, loss: 0.03894126368686557\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 325, loss: 0.03888574009761214\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 326, loss: 0.03883049916476011\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 327, loss: 0.03877556277438998\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 328, loss: 0.038720916491001844\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 329, loss: 0.03866655984893441\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 330, loss: 0.0386124849319458\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 331, loss: 0.038558701518923044\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 332, loss: 0.03850520448759198\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 333, loss: 0.038451981265097857\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 334, loss: 0.03839903650805354\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 335, loss: 0.03834637440741062\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 336, loss: 0.03829398052766919\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 337, loss: 0.03824185719713569\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 338, loss: 0.03819000441581011\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 339, loss: 0.03813841845840216\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 340, loss: 0.03808710305020213\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 341, loss: 0.03803604794666171\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 342, loss: 0.037985255010426044\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 343, loss: 0.03793471958488226\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 344, loss: 0.0378844472579658\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 345, loss: 0.03783442033454776\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 346, loss: 0.03778465045616031\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 347, loss: 0.03773513436317444\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 348, loss: 0.03768586693331599\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 349, loss: 0.03763684583827853\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 350, loss: 0.03758807061240077\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 351, loss: 0.03753954125568271\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 352, loss: 0.03749125171452761\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 353, loss: 0.03744320385158062\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 354, loss: 0.03739538975059986\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 355, loss: 0.03734782012179494\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 356, loss: 0.03730047447606921\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 357, loss: 0.037253370974212885\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 358, loss: 0.03720649937167764\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 359, loss: 0.037159846629947424\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 360, loss: 0.037113435566425323\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 361, loss: 0.037067238707095385\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 362, loss: 0.037021271884441376\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 363, loss: 0.0369755276478827\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 364, loss: 0.036930004600435495\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 365, loss: 0.03688469575718045\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 366, loss: 0.03683961136266589\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 367, loss: 0.036794747691601515\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 368, loss: 0.036750093568116426\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 369, loss: 0.03670565132051706\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 370, loss: 0.036661419086158276\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 371, loss: 0.0366173954680562\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 372, loss: 0.036573588848114014\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 373, loss: 0.03652998572215438\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 374, loss: 0.03648658515885472\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 375, loss: 0.036443385761231184\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 376, loss: 0.036400394048541784\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 377, loss: 0.03635760163888335\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 378, loss: 0.03631501505151391\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 379, loss: 0.03627261985093355\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 380, loss: 0.036230424884706736\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 381, loss: 0.03618842689320445\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 382, loss: 0.03614661982282996\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 383, loss: 0.036105002742260695\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 384, loss: 0.03606357332319021\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 385, loss: 0.036022346932440996\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 386, loss: 0.035981299821287394\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 387, loss: 0.03594044502824545\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 388, loss: 0.035899769980460405\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 389, loss: 0.03585928771644831\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 390, loss: 0.035818986129015684\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 391, loss: 0.035778868943452835\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 392, loss: 0.03573892870917916\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 393, loss: 0.03569917054846883\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 394, loss: 0.03565959073603153\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 395, loss: 0.0356201846152544\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 396, loss: 0.0355809610337019\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 397, loss: 0.03554191021248698\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 398, loss: 0.03550302796065807\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 399, loss: 0.03546432591974735\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 400, loss: 0.035425787791609764\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 401, loss: 0.035387429874390364\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 402, loss: 0.03534922981634736\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 403, loss: 0.03531120205298066\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 404, loss: 0.035273339599370956\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 405, loss: 0.035235646180808544\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 406, loss: 0.03519811388105154\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 407, loss: 0.035160749685019255\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 408, loss: 0.03512354427948594\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 409, loss: 0.03508649254217744\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 410, loss: 0.03504961123690009\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 411, loss: 0.03501288825646043\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 412, loss: 0.03497631987556815\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 413, loss: 0.034939908888190985\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 414, loss: 0.03490365715697408\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 415, loss: 0.034867554903030396\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 416, loss: 0.03483161702752113\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 417, loss: 0.034795820247381926\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 418, loss: 0.03476017713546753\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 419, loss: 0.03472468722611666\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 420, loss: 0.03468935331329703\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 421, loss: 0.034654161892831326\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 422, loss: 0.034619121346622705\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 423, loss: 0.034584220964461565\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 424, loss: 0.034549469128251076\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 425, loss: 0.03451486863195896\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 426, loss: 0.034480401780456305\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 427, loss: 0.03444609045982361\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 428, loss: 0.03441191604360938\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 429, loss: 0.03437788272276521\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 430, loss: 0.03434399701654911\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 431, loss: 0.03431023797020316\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 432, loss: 0.03427662793546915\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 433, loss: 0.03424314735457301\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 434, loss: 0.03420981531962752\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 435, loss: 0.034176604356616735\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 436, loss: 0.03414354287087917\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 437, loss: 0.03411060757935047\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 438, loss: 0.03407781198620796\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 439, loss: 0.034045142121613026\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 440, loss: 0.03401261195540428\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 441, loss: 0.03398020751774311\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 442, loss: 0.03394793486222625\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 443, loss: 0.03391578746959567\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 444, loss: 0.033883774653077126\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 445, loss: 0.03385188477113843\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 446, loss: 0.033820128068327904\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 447, loss: 0.033788494765758514\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 448, loss: 0.03375698672607541\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 449, loss: 0.03372560767456889\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 450, loss: 0.033694347366690636\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 451, loss: 0.03366320813074708\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 452, loss: 0.03363219602033496\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 453, loss: 0.033601312432438135\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 454, loss: 0.0335705429315567\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 455, loss: 0.03353988705202937\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 456, loss: 0.033509362023323774\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 457, loss: 0.03347894921898842\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 458, loss: 0.033448657020926476\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 459, loss: 0.0334184761159122\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 460, loss: 0.03338842326775193\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 461, loss: 0.03335847472772002\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 462, loss: 0.033328655175864697\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 463, loss: 0.03329893993213773\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 464, loss: 0.033269337844103575\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 465, loss: 0.0332398503087461\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 466, loss: 0.033210481982678175\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 467, loss: 0.03318121936172247\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 468, loss: 0.033152072224766016\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 469, loss: 0.03312302893027663\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 470, loss: 0.03309410251677036\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 471, loss: 0.03306528506800532\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 472, loss: 0.033036574721336365\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 473, loss: 0.03300797659903765\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 474, loss: 0.032979479525238276\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 475, loss: 0.032951084431260824\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 476, loss: 0.03292280109599233\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 477, loss: 0.032894632779061794\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 478, loss: 0.03286655526608229\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 479, loss: 0.03283859230577946\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 480, loss: 0.03281072760000825\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 481, loss: 0.032782965805381536\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 482, loss: 0.03275530831888318\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 483, loss: 0.032727754674851894\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 484, loss: 0.03270029369741678\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 485, loss: 0.0326729454100132\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 486, loss: 0.03264568652957678\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 487, loss: 0.03261853661388159\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 488, loss: 0.03259148262441158\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 489, loss: 0.03256453201174736\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 490, loss: 0.03253767127171159\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 491, loss: 0.032510911114513874\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 492, loss: 0.03248424641788006\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 493, loss: 0.032457676250487566\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 494, loss: 0.03243120852857828\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 495, loss: 0.03240483347326517\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 496, loss: 0.03237855341285467\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 497, loss: 0.03235236182808876\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 498, loss: 0.0323262675665319\n",
            "Test_acc: 1.0\n",
            "--------------------------\n",
            "Epoch 499, loss: 0.032300271559506655\n",
            "Test_acc: 1.0\n",
            "--------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}